{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7620d7-05d4-49be-9f4c-9dab1e25f17d",
   "metadata": {},
   "source": [
    "# ðŸš€ Deploy Qwen2.5 Coder-3B-Instruct Model on Amazon SageMaker AI using LMI\n",
    "\n",
    "## Introduction: [Qwen2.5 Coder 3B](https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct)\n",
    "\n",
    "Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n",
    "\n",
    "Significantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\n",
    "A more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n",
    "This repo contains the instruction-tuned 3B Qwen2.5-Coder model, which has the following features:\n",
    "\n",
    "Type: Causal Language Models\n",
    "Training Stage: Pretraining & Post-training\n",
    "Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n",
    "Number of Parameters: 3.09B\n",
    "Number of Paramaters (Non-Embedding): 2.77B\n",
    "Number of Layers: 36\n",
    "Number of Attention Heads (GQA): 16 for Q and 2 for KV\n",
    "Context Length: Full 32,768 tokens\n",
    "For more details, please refer to our blog, GitHub, Documentation, Arxiv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0495a9c3-93c3-450f-82d7-6d91aa30492d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T10:56:48.549212Z",
     "iopub.status.busy": "2025-07-07T10:56:48.548872Z",
     "iopub.status.idle": "2025-07-07T10:57:14.708861Z",
     "shell.execute_reply": "2025-07-07T10:57:14.708164Z",
     "shell.execute_reply.started": "2025-07-07T10:56:48.549188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq sagemaker boto3 huggingface_hub --force-reinstall --no-cache-dir --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49095ae2-0f51-49e3-acf4-e554fff52cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T10:57:14.710193Z",
     "iopub.status.busy": "2025-07-07T10:57:14.709905Z",
     "iopub.status.idle": "2025-07-07T10:57:17.019780Z",
     "shell.execute_reply": "2025-07-07T10:57:17.019152Z",
     "shell.execute_reply.started": "2025-07-07T10:57:14.710168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "prefix: DEMO-1751885837-6db5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import (\n",
    "    HuggingFaceModel, \n",
    "    get_huggingface_llm_image_uri\n",
    ")\n",
    "\n",
    "boto_region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "model_bucket = 'practiceb22' #sagemaker_session.default_bucket()  # bucket to house artifacts\n",
    "s3_model_prefix = (\n",
    "    \"qwenmodels\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "print(f\"prefix: {prefix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66560cdc-9dda-478a-bac2-88ed2e6feef3",
   "metadata": {},
   "source": [
    "## Setup your SageMaker Real-time Endpoint \n",
    "### Create a SageMaker endpoint configuration\n",
    "\n",
    "We begin by creating the endpoint configuration and set MinInstanceCount to 0. This allows the endpoint to scale in all the way down to zero instances when not in use. See the [notebook example for SageMaker AI endpoint scale down to zero](https://github.com/aws-samples/sagemaker-genai-hosting-examples/tree/02236395d44cf54c201eefec01fd8da0a454092d/scale-to-zero-endpoint).\n",
    "\n",
    "There are a few parameters we want to setup for our endpoint. We first start by setting the variant name, and instance type we want our endpoint to use. In addition we set the *model_data_download_timeout_in_seconds* and *container_startup_health_check_timeout_in_seconds* to have some guardrails for when we deploy inference components to our endpoint. In addition we will use Managed Instance Scaling which allows SageMaker to scale the number of instances based on the requirements of the scaling of your inference components. We set a *MinInstanceCount* and *MinInstanceCount* variable to size this according to the workload you want to service and also maintain controls around cost. Lastly, we set *RoutingStrategy* for the endpoint to optimally tune how to route requests to instances and inference components for the best performance.\n",
    "\n",
    "The suggested instance types to host the QwQ 30B model can be `ml.g5.24xlarge`, `ml.g6.12xlarge`, `ml.g6e.12xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6322e5d-23ad-4eb1-9d50-c88bd0ee4996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set an unique endpoint config name\n",
    "endpoint_config_name = f\"{prefix}-endpoint-config\"\n",
    "print(f\"Demo endpoint config name: {endpoint_config_name}\")\n",
    "\n",
    "# Set varient name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0 # Minimum instance must be set to 0\n",
    "max_instance_count = 2\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": min_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7448b-0e70-4bfa-9d39-31757c19de6f",
   "metadata": {},
   "source": [
    "### Create the SageMaker endpoint\n",
    "Next, we create our endpoint using the above endpoint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e0182ef-7f2b-4ded-b407-a8411bd391e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T11:44:12.584129Z",
     "iopub.status.busy": "2025-07-07T11:44:12.583792Z",
     "iopub.status.idle": "2025-07-07T11:44:12.998510Z",
     "shell.execute_reply": "2025-07-07T11:44:12.997939Z",
     "shell.execute_reply.started": "2025-07-07T11:44:12.584106Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo endpoint name: DEMO-1751885837-6db5-endpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:ap-south-1:572285620711:endpoint/DEMO-1751885837-6db5-endpoint',\n",
       " 'ResponseMetadata': {'RequestId': '2f2641f3-44ae-4c1b-b4c8-838468814c98',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2f2641f3-44ae-4c1b-b4c8-838468814c98',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '98',\n",
       "   'date': 'Mon, 07 Jul 2025 11:44:12 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"{prefix}-endpoint\"\n",
    "print(f\"Demo endpoint name: {endpoint_name}\")\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b91c3d36-138d-4aa1-b334-ae347bd6f13f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T11:44:17.187013Z",
     "iopub.status.busy": "2025-07-07T11:44:17.186676Z",
     "iopub.status.idle": "2025-07-07T11:47:47.645803Z",
     "shell.execute_reply": "2025-07-07T11:47:47.645289Z",
     "shell.execute_reply.started": "2025-07-07T11:44:17.186990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'DEMO-1751885837-6db5-endpoint',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:ap-south-1:572285620711:endpoint/DEMO-1751885837-6db5-endpoint',\n",
       " 'EndpointConfigName': 'DEMO-1751885837-6db5-endpoint-config',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1,\n",
       "   'ManagedInstanceScaling': {'Status': 'ENABLED',\n",
       "    'MinInstanceCount': 0,\n",
       "    'MaxInstanceCount': 2},\n",
       "   'RoutingConfig': {'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'}}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2025, 7, 7, 11, 44, 12, 968000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2025, 7, 7, 11, 47, 17, 664000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'cad40cae-5618-47c2-aba7-63d08b06de88',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'cad40cae-5618-47c2-aba7-63d08b06de88',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '557',\n",
       "   'date': 'Mon, 07 Jul 2025 11:47:47 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab9858-7545-4d63-8431-a806acc03391",
   "metadata": {},
   "source": [
    "## Deploy using Amazon SageMaker Large Model Inference (LMI) container \n",
    "In this example we are goign to use the LMI v15 container powered by vLLM 0.8.4 with support for the vLLM V1 engine. This version now supports the latest open-source models, such as Metaâ€™s Llama 4 models Scout and Maverick, Googleâ€™s Gemma 3, Alibabaâ€™s Qwen, Mistral AI, DeepSeek-R, and many more. You can find more details of the LMI v15 container from [the blog here](https://aws.amazon.com/blogs/machine-learning/supercharge-your-llm-performance-with-amazon-sagemaker-large-model-inference-container-v15/).\n",
    "\n",
    "\n",
    "\n",
    "### Create Model Artifact\n",
    "We will be deploying the Qwen 30B A3B model using the LMI container. In order to do so you need to set the image you would like to use with the proper configuartion. You can also create a SageMaker model to be referenced when you create your inference component\n",
    "\n",
    "#### Download the model from Hugging Face and upload the model artifacts on Amazon S3\n",
    "In this example, we will demonstrate how to download your copy of the model from huggingface and upload it to an s3 location in your AWS account, then deploy the model with the downloaded model artifacts to an endpoint. \n",
    "\n",
    "First, download the model artifact data from HuggingFace. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61dd4746-be89-46ad-8b5f-67203db639ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T10:59:47.940678Z",
     "iopub.status.busy": "2025-07-07T10:59:47.940169Z",
     "iopub.status.idle": "2025-07-07T11:00:28.900227Z",
     "shell.execute_reply": "2025-07-07T11:00:28.899658Z",
     "shell.execute_reply.started": "2025-07-07T10:59:47.940655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d277d712ebdc4eabb276544e2fe996bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13966548388b489c96f6cc8ce3406acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057f1cb29917449cbd53edde16929c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3191fba5d49d4b16a3af44e7618faaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d443effbd343cf8af56a0eb776f178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61a7591ebf24d89ba9a6c306b48b4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb18a4cdef64210a05e2982130043ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b54a91ed8f457f95a19dbfe0cfcfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.21G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d99d33399f4f8d8a46343888c012a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a29e15a4cd44a08c90b41f90a27368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sagemaker\n",
    "import jinja2\n",
    "\n",
    "qwen25_3B = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\".\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = qwen25_3B\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.safetensors\", \"*.bin\", \"*.txt\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "631ad04c-54ce-4af2-b745-b3a149a7676e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T11:00:28.901337Z",
     "iopub.status.busy": "2025-07-07T11:00:28.900965Z",
     "iopub.status.idle": "2025-07-07T11:00:28.904751Z",
     "shell.execute_reply": "2025-07-07T11:00:28.904255Z",
     "shell.execute_reply.started": "2025-07-07T11:00:28.901317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model will be uploaded to ---- > s3://practiceb22/qwenmodels/\n"
     ]
    }
   ],
   "source": [
    "# define a variable to contain the s3url of the location that has the model\n",
    "pretrained_model_location = f\"s3://{model_bucket}/{s3_model_prefix}/\"\n",
    "print(f\"Pretrained model will be uploaded to ---- > {pretrained_model_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd26dbd-9c82-4722-a22d-77e46ed25c1e",
   "metadata": {},
   "source": [
    "Upload model data to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "968d9b5b-391a-4309-9434-9e61dd365725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T11:40:34.545863Z",
     "iopub.status.busy": "2025-07-07T11:40:34.545533Z",
     "iopub.status.idle": "2025-07-07T11:40:34.549950Z",
     "shell.execute_reply": "2025-07-07T11:40:34.549435Z",
     "shell.execute_reply.started": "2025-07-07T11:40:34.545842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models--Qwen--Qwen2.5-Coder-3B-Instruct/snapshots/488639f1ff808d1d3d0ba301aef8c11461451ec5'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_download_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "066f46e4-474b-40da-9fd9-5edb367af285",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T11:41:23.129925Z",
     "iopub.status.busy": "2025-07-07T11:41:23.129360Z",
     "iopub.status.idle": "2025-07-07T11:42:06.008014Z",
     "shell.execute_reply": "2025-07-07T11:42:06.007434Z",
     "shell.execute_reply.started": "2025-07-07T11:41:23.129903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to --- > s3://practiceb22/qwenmodels\n",
      "We will set option.s3url=s3://practiceb22/qwenmodels\n"
     ]
    }
   ],
   "source": [
    "model_artifact = sagemaker_session.upload_data(path=model_download_path, bucket = model_bucket, key_prefix=s3_model_prefix)\n",
    "print(f\"Model uploaded to --- > {model_artifact}\")\n",
    "print(f\"We will set option.s3url={model_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3731b3f4-744f-46a1-b9d1-6e206cc39360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "# !rm -rf {model_download_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f26a4-7591-47e4-af4e-990c59079928",
   "metadata": {},
   "source": [
    "To find our more of the SageMaker `create_model` api call, you can see the details in [the boto3 doc](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_model.html). Note that you can use the **CompressionType** to specify how the model data is prepared.  \n",
    "\n",
    "If you choose `Gzip` and choose `S3Object` as the value of `S3DataType`, `S3Uri` identifies an object that is a gzip-compressed TAR archive. SageMaker will attempt to decompress and untar the object during model deployment.\n",
    "\n",
    "If you choose `None` and `S3Prefix` as the value of `S3DataType`, then for each S3 object under the key name pefix referenced by `S3Uri`, SageMaker will trim its key by the prefix, and use the remainder as the path (relative to `/opt/ml/model`) of the file holding the content of the S3 object. SageMaker will split the remainder by slash (/), using intermediate parts as directory names and the last part as filename of the file holding the content of the S3 object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd27d45d-0624-48f2-af2f-a322e41601a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T11:48:04.413944Z",
     "iopub.status.busy": "2025-07-07T11:48:04.413615Z",
     "iopub.status.idle": "2025-07-07T11:48:05.118989Z",
     "shell.execute_reply": "2025-07-07T11:48:05.118383Z",
     "shell.execute_reply.started": "2025-07-07T11:48:04.413923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelArn': 'arn:aws:sagemaker:ap-south-1:572285620711:model/qwen2-5-coder-3b-tgi-250707-114804',\n",
       " 'ResponseMetadata': {'RequestId': 'd8799bb8-1192-49e3-a29f-e4a2b3b5b0b6',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd8799bb8-1192-49e3-a29f-e4a2b3b5b0b6',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '97',\n",
       "   'date': 'Mon, 07 Jul 2025 11:48:05 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define region where you have capacity\n",
    "REGION = boto_region\n",
    "\n",
    "#Select the latest container. Check the link for the latest available version https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers \n",
    "CONTAINER_VERSION = '0.33.0-lmi15.0.0-cu128'\n",
    "\n",
    "# Construct container URI\n",
    "container_uri = f'763104351884.dkr.ecr.{REGION}.amazonaws.com/djl-inference:{CONTAINER_VERSION}'\n",
    "\n",
    "pretrained_model_location = f\"s3://{model_bucket}/{s3_model_prefix}/\"\n",
    "qwen2_5_model = {\n",
    "    \"Image\": container_uri,\n",
    "    'ModelDataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3Uri': pretrained_model_location,\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'CompressionType': 'None',\n",
    "                }\n",
    "            },\n",
    "    \"Environment\": {\n",
    "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
    "        \"MESSAGES_API_ENABLED\": \"true\",\n",
    "        \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"4\",\n",
    "        \"OPTION_MODEL_LOADING_TIMEOUT\": \"1500\",\n",
    "        \"SERVING_FAIL_FAST\": \"true\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "        \"OPTION_ASYNC_MODE\": \"true\",\n",
    "        \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
    "        \"OPTION_ENABLE_STREAMING\": \"true\"\n",
    "    },\n",
    "}\n",
    "model_name_qwen2_5 = f\"qwen2-5-coder-3b-tgi-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "# create SageMaker Model\n",
    "sagemaker_client.create_model(\n",
    "    ModelName=model_name_qwen2_5,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[qwen2_5_model],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e6094-9ea6-4481-be2e-9ee2d00b92bd",
   "metadata": {},
   "source": [
    "We can now create the Inference Components which will deployed on the endpoint that you specify. Please note here that you can provide a SageMaker model or a container to specification. If you provide a container, you will need to provide an image and artifactURL as parameters. In this example we set it to the model name we prepared in the cells above. You can also set the `ComputeResourceRequirements` to supply SageMaker what should be reserved for each copy of the inference component. You can also set the copy count of the number of Inference Components you would like to deploy. These can be managed and scaled as the capabilities become available. \n",
    "\n",
    "Note that in this example we set the `NumberOfAcceleratorDevicesRequired` to a value of `4`. By doing so we reserve 4 accelerators for each copy of this inference component so that we can use tensor parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42fc8b52-e2d1-4327-b4fc-8f7d8c740bad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T11:48:11.006214Z",
     "iopub.status.busy": "2025-07-07T11:48:11.005901Z",
     "iopub.status.idle": "2025-07-07T11:48:11.556547Z",
     "shell.execute_reply": "2025-07-07T11:48:11.555962Z",
     "shell.execute_reply.started": "2025-07-07T11:48:11.006191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'InferenceComponentArn': 'arn:aws:sagemaker:ap-south-1:572285620711:inference-component/DEMO-1751885837-6db5-IC-qwen3-30b-250707-114811',\n",
       " 'ResponseMetadata': {'RequestId': '34186504-be2b-431f-8acc-f3e8d2670d83',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '34186504-be2b-431f-8acc-f3e8d2670d83',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '137',\n",
       "   'date': 'Mon, 07 Jul 2025 11:48:11 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_component_name_qwen = f\"{prefix}-IC-qwen3-30b-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name_qwen2_5,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cd376f-d3f5-4f8e-a4b5-ea11ce9fa60e",
   "metadata": {},
   "source": [
    "Wait until the inference component is `InService`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56190add-87e3-4081-ae69-2d603911522e",
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-08T03:06:03.355Z",
     "iopub.execute_input": "2025-07-07T11:48:17.020115Z",
     "iopub.status.busy": "2025-07-07T11:48:17.019780Z",
     "iopub.status.idle": "2025-07-07T11:56:49.263987Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "InService\n",
      "\n",
      "Total time taken: 512.24 seconds (8.54 minutes)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Let's see how much it takes\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name_qwen\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e285c-b98c-4e23-87dd-739898c32dec",
   "metadata": {},
   "source": [
    "#### Invoke endpoint with boto3\n",
    "Now you can invoke the endpoint with boto3 `invoke_endpoint` or `invoke_endpoint_with_response_stream` runtime api calls. If you have an existing endpoint, you don't need to recreate the `predictor` and can follow below example to invoke the endpoint with an endpoint name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6817557b-8204-4171-bcaa-54395f8b8868",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T05:09:56.748047Z",
     "iopub.status.busy": "2025-07-14T05:09:56.747712Z",
     "iopub.status.idle": "2025-07-14T05:10:03.678840Z",
     "shell.execute_reply": "2025-07-14T05:10:03.678267Z",
     "shell.execute_reply.started": "2025-07-14T05:09:56.748025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! In Python, you can convert a list to JSON using the `json` module. Here's a simple example:\n",
      "\n",
      "```python\n",
      "import json\n",
      "\n",
      "# Example list\n",
      "my_list = [\n",
      "    {\"name\": \"Alice\", \"age\": 30},\n",
      "    {\"name\": \"Bob\", \"age\": 25},\n",
      "    {\"name\": \"Charlie\", \"age\": 35}\n",
      "]\n",
      "\n",
      "# Convert list to JSON\n",
      "json_string = json.dumps(my_list)\n",
      "\n",
      "# Print the JSON string\n",
      "print(json_string)\n",
      "```\n",
      "\n",
      "This code will output:\n",
      "\n",
      "```json\n",
      "[{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}, {\"name\": \"Charlie\", \"age\": 35}]\n",
      "```\n",
      "\n",
      "The `json.dumps()` function converts a Python object into a JSON formatted string. If you want to write the JSON data to a file instead of printing it, you can use `json.dump()`, which writes the JSON representation of the object to a file:\n",
      "\n",
      "```python\n",
      "import json\n",
      "\n",
      "# Example list\n",
      "my_list = [\n",
      "    {\"name\": \"Alice\", \"age\": 30},\n",
      "    {\"name\": \"Bob\", \"age\": 25},\n",
      "    {\"name\": \"Charlie\", \"age\": 35}\n",
      "]\n",
      "\n",
      "# Write JSON to a file\n",
      "with open('output.json', 'w') as file:\n",
      "    json.dump(my_list, file, indent=4)\n",
      "```\n",
      "\n",
      "This will create a file named `output.json` with the following content:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"Alice\",\n",
      "        \"age\": 30\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Bob\",\n",
      "        \"age\": 25\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Charlie\",\n",
      "        \"age\": 35\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "The `indent=4` argument is optional and is used to format the JSON output with indentation for better readability.\n",
      "6.9225451946258545\n",
      "{'prompt_tokens': 37, 'total_tokens': 440, 'completion_tokens': 403, 'prompt_tokens_details': None}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "prompt = {\n",
    "    'messages':[\n",
    "    {\"role\": \"user\", \"content\": \"Generate a code to convert list to json\"}\n",
    "],\n",
    "    'temperature':0.7,\n",
    "    'top_p':0.8,\n",
    "    'top_k':20,\n",
    "    'max_tokens':512,\n",
    "    #'system': 'Just share the code and no examples or explanation'\n",
    "}\n",
    "\n",
    "system_prompt = 'Just share the code and no examples or explanation'\n",
    "# Create the payload\n",
    "payload = {\n",
    "    \"inputs\": 'Generate a code to convert list to json',\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"temperature\": 0.7,\n",
    "        \"do_sample\": False\n",
    "    },\n",
    "    \"system\": system_prompt  # Adding system prompt to the request\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "end_time = time.time()\n",
    "overall_latency = end_time - start_time\n",
    "\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)\n",
    "print(overall_latency)\n",
    "\n",
    "model_usage = response_content = response_dict['usage']\n",
    "print(model_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3dc662a6-008c-481d-b897-910ff2363f43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T05:10:03.680512Z",
     "iopub.status.busy": "2025-07-14T05:10:03.679926Z",
     "iopub.status.idle": "2025-07-14T05:10:03.684629Z",
     "shell.execute_reply": "2025-07-14T05:10:03.683965Z",
     "shell.execute_reply.started": "2025-07-14T05:10:03.680480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '77cb1759-650b-42fd-bbe9-7cea62ef990a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '77cb1759-650b-42fd-bbe9-7cea62ef990a',\n",
       "   'x-amzn-invoked-production-variant': 'AllTraffic',\n",
       "   'date': 'Mon, 14 Jul 2025 05:10:03 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '1902',\n",
       "   'connection': 'keep-alive'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ContentType': 'application/json',\n",
       " 'InvokedProductionVariant': 'AllTraffic',\n",
       " 'Body': <botocore.response.StreamingBody at 0x7fce23d59300>}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f3af818-416e-42e9-9aca-75ae0e5e1df9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T05:10:08.494050Z",
     "iopub.status.busy": "2025-07-14T05:10:08.493730Z",
     "iopub.status.idle": "2025-07-14T05:10:08.498386Z",
     "shell.execute_reply": "2025-07-14T05:10:08.497821Z",
     "shell.execute_reply.started": "2025-07-14T05:10:08.494027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-7e5255922ed2436f8091a794d204c9ff',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1752469796,\n",
       " 'model': 'lmi',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'reasoning_content': None,\n",
       "    'content': 'Certainly! In Python, you can convert a list to JSON using the `json` module. Here\\'s a simple example:\\n\\n```python\\nimport json\\n\\n# Example list\\nmy_list = [\\n    {\"name\": \"Alice\", \"age\": 30},\\n    {\"name\": \"Bob\", \"age\": 25},\\n    {\"name\": \"Charlie\", \"age\": 35}\\n]\\n\\n# Convert list to JSON\\njson_string = json.dumps(my_list)\\n\\n# Print the JSON string\\nprint(json_string)\\n```\\n\\nThis code will output:\\n\\n```json\\n[{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}, {\"name\": \"Charlie\", \"age\": 35}]\\n```\\n\\nThe `json.dumps()` function converts a Python object into a JSON formatted string. If you want to write the JSON data to a file instead of printing it, you can use `json.dump()`, which writes the JSON representation of the object to a file:\\n\\n```python\\nimport json\\n\\n# Example list\\nmy_list = [\\n    {\"name\": \"Alice\", \"age\": 30},\\n    {\"name\": \"Bob\", \"age\": 25},\\n    {\"name\": \"Charlie\", \"age\": 35}\\n]\\n\\n# Write JSON to a file\\nwith open(\\'output.json\\', \\'w\\') as file:\\n    json.dump(my_list, file, indent=4)\\n```\\n\\nThis will create a file named `output.json` with the following content:\\n\\n```json\\n[\\n    {\\n        \"name\": \"Alice\",\\n        \"age\": 30\\n    },\\n    {\\n        \"name\": \"Bob\",\\n        \"age\": 25\\n    },\\n    {\\n        \"name\": \"Charlie\",\\n        \"age\": 35\\n    }\\n]\\n```\\n\\nThe `indent=4` argument is optional and is used to format the JSON output with indentation for better readability.',\n",
       "    'tool_calls': []},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'stop_reason': None}],\n",
       " 'usage': {'prompt_tokens': 37,\n",
       "  'total_tokens': 440,\n",
       "  'completion_tokens': 403,\n",
       "  'prompt_tokens_details': None},\n",
       " 'prompt_logprobs': None}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_dict\n",
    "#response_dict['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015339db-9d8c-481b-bca9-5f7955991a5b",
   "metadata": {},
   "source": [
    "#### Streaming response from the endpoint\n",
    "Additionally, SGLang allows you to invoke the endpoint and receive streaming response. Below is an example of how to interact with the endpoint with streaming response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a65f8ce7-fcf1-4276-a1ae-98dc3ef7638b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T05:10:11.435099Z",
     "iopub.status.busy": "2025-07-14T05:10:11.434652Z",
     "iopub.status.idle": "2025-07-14T05:10:11.441033Z",
     "shell.execute_reply": "2025-07-14T05:10:11.440481Z",
     "shell.execute_reply.started": "2025-07-14T05:10:11.435077Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "# Example class that processes an inference stream:\n",
    "class SmrInferenceStream:\n",
    "    \n",
    "    def __init__(self, sagemaker_runtime, endpoint_name, inference_component_name=None):\n",
    "        self.sagemaker_runtime = sagemaker_runtime\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.inference_component_name = inference_component_name\n",
    "        # A buffered I/O stream to combine the payload parts:\n",
    "        self.buff = io.BytesIO() \n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def stream_inference(self, request_body):\n",
    "        # Gets a streaming inference response \n",
    "        # from the specified model endpoint:\n",
    "        response = self.sagemaker_runtime\\\n",
    "            .invoke_endpoint_with_response_stream(\n",
    "                EndpointName=self.endpoint_name, \n",
    "                InferenceComponentName=self.inference_component_name,\n",
    "                Body=json.dumps(request_body), \n",
    "                ContentType=\"application/json\"\n",
    "        )\n",
    "        # Gets the EventStream object returned by the SDK:\n",
    "        event_stream = response['Body']\n",
    "        for event in event_stream:\n",
    "            # Passes the contents of each payload part\n",
    "            # to be concatenated:\n",
    "            self._write(event['PayloadPart']['Bytes'])\n",
    "            # Iterates over lines to parse whole JSON objects:\n",
    "            for line in self._readlines():\n",
    "                try:\n",
    "                    resp = json.loads(line)\n",
    "                except:\n",
    "                    continue\n",
    "                if len(line)>0 and type(resp) == dict:\n",
    "                    # if len(resp.get('choices')) == 0:\n",
    "                    #     continue\n",
    "                    part = resp.get('choices')[0]['delta']['content']\n",
    "                    \n",
    "                else:\n",
    "                    part = resp\n",
    "                # Returns parts incrementally:\n",
    "                yield part\n",
    "    \n",
    "    # Writes to the buffer to concatenate the contents of the parts:\n",
    "    def _write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "\n",
    "    # The JSON objects in buffer end with '\\n'.\n",
    "    # This method reads lines to yield a series of JSON objects:\n",
    "    def _readlines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            self.read_pos += len(line)\n",
    "            yield line[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40f29fdb-e4be-439c-a6e4-dc05facc6b92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T05:10:12.731050Z",
     "iopub.status.busy": "2025-07-14T05:10:12.730724Z",
     "iopub.status.idle": "2025-07-14T05:10:17.743057Z",
     "shell.execute_reply": "2025-07-14T05:10:17.742437Z",
     "shell.execute_reply.started": "2025-07-14T05:10:12.731027Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! To convert a list to JSON in Python, you can use the `json` module, which is part of the standard library. Here's a simple example:\n",
      "\n",
      "```python\n",
      "import json\n",
      "\n",
      "# Example list\n",
      "my_list = [\n",
      "    {\"name\": \"Alice\", \"age\": 30},\n",
      "    {\"name\": \"Bob\", \"age\": 25},\n",
      "    {\"name\": \"Charlie\", \"age\": 35}\n",
      "]\n",
      "\n",
      "# Convert list to JSON\n",
      "json_output = json.dumps(my_list, indent=4)\n",
      "\n",
      "print(json_output)\n",
      "```\n",
      "\n",
      "In this example, `json.dumps()` is used to convert the list into a JSON-formatted string. The `indent` parameter is optional and is used to pretty-print the JSON output with an indentation of 4 spaces for better readability.\n",
      "\n",
      "If you want to write the JSON data to a file, you can use the `json.dump()` function instead:\n",
      "\n",
      "```python\n",
      "import json\n",
      "\n",
      "# Example list\n",
      "my_list = [\n",
      "    {\"name\": \"Alice\", \"age\": 30},\n",
      "    {\"name\": \"Bob\", \"age\": 25},\n",
      "    {\"name\": \"Charlie\", \"age\": 35}\n",
      "]\n",
      "\n",
      "# Write list to JSON file\n",
      "with open('output.json', 'w') as json_file:\n",
      "    json.dump(my_list, json_file, indent=4)\n",
      "```\n",
      "\n",
      "This will create a file named `output.json` with the JSON representation of your list."
     ]
    }
   ],
   "source": [
    "request_body = {\n",
    "    'messages':[\n",
    "        {\"role\": \"user\", \"content\": \"Generate a code to convert list to json\"},\n",
    "    ],\n",
    "    'temperature':0.9,\n",
    "    'max_tokens':512,\n",
    "    'stream': True,\n",
    "}\n",
    "\n",
    "smr_inference_stream = SmrInferenceStream(\n",
    "    sagemaker_runtime, endpoint_name, inference_component_name_qwen)\n",
    "stream = smr_inference_stream.stream_inference(request_body)\n",
    "for part in stream:\n",
    "    print(part, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacb334-1383-4bab-a118-683d3328b929",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "  \n",
    "Make sure to delete the endpoint and other artifacts that were created to avoid unnecessary cost. You can also go to SageMaker AI console to delete all the resources created in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71881f-1458-4557-9a13-9d764377e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name_qwen)\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f12ce-8c71-47c6-a741-2b1286d68fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
